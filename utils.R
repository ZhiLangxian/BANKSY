# Function to install missing packages from GitHub with dependencies and upgrade option
install_missing_github_fx <- function(pkg_name, github_repo, use_remotes = FALSE, dependencies = TRUE, upgrade = "always") {
  # Initialize a flag to track installation
  installed <- FALSE
  
  if (!requireNamespace(pkg_name, quietly = TRUE)) {
    message(paste("Installing", pkg_name, "from", github_repo))
    
    # Ensure the installer package is available
    if (use_remotes) {
      if (!requireNamespace("remotes", quietly = TRUE)) {
        install.packages("remotes")
      }
    } else {
      if (!requireNamespace("devtools", quietly = TRUE)) {
        install.packages("devtools")
      }
    }
    
    # Attempt to install the package and handle potential errors
    tryCatch({
      if (use_remotes) {
        remotes::install_github(github_repo, dependencies = dependencies, upgrade = upgrade, quiet = TRUE)
      } else {
        devtools::install_github(github_repo, dependencies = dependencies, upgrade = upgrade, quiet = TRUE)
      }
      installed <- TRUE  # Set flag to TRUE if installation succeeds
      message(paste(pkg_name, "installed successfully"))
    }, error = function(e) {
      message(paste("Failed to install", pkg_name, ":", e$message))
    })
  } else {
    message(paste(pkg_name, "is already installed"))
  }
  
  return(installed)  # Ensure the function returns TRUE or FALSE
}

# Function for smoothing according to the neighbours in the UMAP space
  # From the Nanostring ScratchSpace CosMx analysis vignette: 
  # https://nanostring-biostats.github.io/CosMx-Analysis-Scratch-Space/posts/marker-gene-smoothing/#introduction
umap_nn_fx <- function(sem, umapreduc, n_neighbors=100){
  
  ## extract umap coordinates 
  umapd <-  
    data.table(sem@reductions[[umapreduc]]@cell.embeddings
               ,keep.rownames = TRUE)
  setnames(umapd, c(names(umapd)[1:3]), c("cell_ID", "UMAP_1", "UMAP_2"))
  
  ## identify nearest n_neighbors (+1 includes the cell as a neighbor to itself)
  nn_umap <- RANN::nn2(umapd[,.(UMAP_1, UMAP_2)],k = n_neighbors + 1)$nn.idx
  nn_umap <- data.table::melt(cbind(umapd[,.(cell_ID)], data.table(nn_umap))
                              , id.vars=c("cell_ID", "V1"))
  colnames(nn_umap) <- c("cell_ID1", "cell_ID1_idx", "neighbor", "cell_ID2_idx")
  nn_umap <- merge(nn_umap
                   , nn_umap[,.(cell_ID2=cell_ID1, cell_ID2_idx=cell_ID1_idx)][
                     ,unique(.SD)],by="cell_ID2_idx")
  
  ## Cell x cell neighbor indicator matrix
  wumap <- Matrix::sparseMatrix(i = c(unique(nn_umap$cell_ID1_idx), nn_umap$cell_ID2_idx)
                                ,j=c(unique(nn_umap$cell_ID1_idx), nn_umap$cell_ID1_idx)
                                ,x=1)
  
  rownames(wumap) <- colnames(wumap) <- nn_umap[order(nn_umap$cell_ID1_idx),unique(cell_ID1)]
  
  ## Column standardize, so that columns (cells) sum to 1, and each neighbor given equal weight.
  mumap <- Matrix::sparseMatrix(i=1:ncol(wumap)
                                ,j=1:ncol(wumap)
                                ,x=1/Matrix::colSums(wumap)
  )
  dimnames(mumap) <- dimnames(wumap)
  smoother <- mumap %*% wumap
  smoother <- smoother[,colnames(sem)]
  return(smoother)
}

# Function to extract unique marker genes as vectors to use for heatmaps or dot plots
  # Starts from a dataframe generated by Seurat::FindAllMarkers()
extract_and_clean_genes_fx <- function(data, gene_col = "gene", cluster_col = "cluster", top_n = 10) {
  # Extract and collapse top n genes per cluster
  genes_per_cluster <- tapply(data[[gene_col]], list(data[[cluster_col]]), function(i) paste0(i[1:top_n], collapse = ","))
  
  # Unname and unlist the collapsed gene strings
  genes_unlisted <- unname(unlist(strsplit(unlist(genes_per_cluster), ",")))
  
  # Remove "NA" strings and actual NA values
  genes_cleaned <- genes_unlisted[genes_unlisted != "NA"]
  genes_cleaned <- genes_cleaned[!is.na(genes_cleaned)]
  
  return(genes_cleaned)
}

# Define the function to subset and count cells
  # To prepare for running dittoHeatmap at single cell level

subset_and_count_cells_fx <- function(spe, cluster_col) {
  # Extract unique clusters
  unique_clusters <- unique(colData(spe)[[cluster_col]])
  
  # Create a named vector to store the number of cells in each subset1
  cell_counts <- numeric(length(unique_clusters))
  names(cell_counts) <- unique_clusters
  
  # subset the SpatialExperiment object by each unique cluster and count cells
  for (cluster in unique_clusters) {
    subset1_spe <- spe[, colData(spe)[[cluster_col]] == cluster]
    cell_counts[as.character(cluster)] <- ncol(subset1_spe)
  }
  
  return(cell_counts)
}

# Define the function to sort and compute cumulative counts
  # To prepare for running dittoHeatmap at single cell level

sort_and_cumulate_fx <- function(cell_counts, matching_values_sort) {
  # Sort the cell counts in descending order
  sorted_cell_counts <- cell_counts[match(matching_values_sort, names(cell_counts))]
  
  # Add the previous element's cell count to each element
  cumulative_counts <- sorted_cell_counts
  for (i in 2:length(sorted_cell_counts)) {
    cumulative_counts[i] <- cumulative_counts[i] + cumulative_counts[i - 1]
  }
  
  return(cumulative_counts)
}

# Function to create beautiful radar chart
  ## codes from datanova https://www.datanovia.com/en/blog/beautiful-radar-chart-in-r-using-fmsb-and-ggplot-packages/ 
create_beautiful_radarchart_fx <- function(data, color = "#00AFBB", 
                                        vlabels = colnames(data), vlcex = 0.7,
                                        caxislabels = NULL, title = NULL, ...){
  radarchart(
    data, axistype = 1,
    # Customize the polygon
    pcol = color, pfcol = scales::alpha(color, 0.5), plwd = 2, plty = 1,
    # Customize the grid
    cglcol = "grey", cglty = 1, cglwd = 0.8,
    # Customize the axis
    axislabcol = "grey", 
    # Variable labels
    vlcex = vlcex, vlabels = vlabels,
    caxislabels = caxislabels, title = title, ...
  )
}

output_dir_svg_plots_fx <- function(ts) {
  # Use getwd() to get the output directory base
  output_dir_base <- getwd()
  
  # Use knitr::current_input() to get the R Markdown filename without extension
  if (!is.null(knitr::current_input())) {
    md_filename <- tools::file_path_sans_ext(basename(knitr::current_input()))
  } else {
    # Fallback filename for interactive sessions
    md_filename <- "interactive_session"
  }
  
  # Define the output directory path
  output_dir <- file.path(output_dir_base, paste0(md_filename, "_svg_files_", ts))
  
  # Create the directory without prompts
  if (!dir.exists(output_dir)) {
    dir.create(
      output_dir,
      recursive = TRUE,
      showWarnings = FALSE
    )
  }
  
  # Return the directory path
  return(output_dir)
}

# Define a function to return common plot components to use with boxplots
get_common_boxplot_components_fx <- function() {
  list(
    geom_point(position = position_dodge2(width = 0.5, padding = 1.5)),
    theme_minimal(),
    theme(
      axis.text.x = element_text(size = 14, face = "bold", angle = 45, hjust = 1),
      axis.text.y = element_text(size = 14, face = "bold"),
      legend.text = element_text(size = 14, face = "bold"),
      axis.title = element_text(size = 14, face = "bold"),
      legend.title = element_text(size = 14, face = "bold"),
      plot.title = element_text(size = 16, face = "bold"),
      strip.text = element_text(size = 14, face = "bold")
    )
  )
}

# Define a function to return common plot components to use with UMAP plots 
get_common_UMAP_components_fx <- function(p) {
  p +
    guides(color = guide_legend(override.aes = list(size = 5), # Adjust legend symbol size
                                title = NULL)) +  # Remove legend title
    theme(
      legend.text = element_text(size = 16),
      # Legend text size
      legend.key.size = unit(1.5, 'lines'),
      # Adjust legend symbol size
      axis.text.x = element_text(size = 16),
      # X-axis tick labels size
      axis.text.y = element_text(size = 16),
      # Y-axis tick labels size
      axis.title.x = element_text(size = 16),
      # X-axis title size
      axis.title.y = element_text(size = 16)        # Y-axis title size
    ) +
    coord_fixed(ratio = 1)  # Fix the aspect ratio
}

# Sanitize a filename by replacing spaces with underscores, removing or replacing special characters
# optionally removing other non-alphanumeric characters
sanitize_filename_fx <- function(filename) {
  # Replace spaces with underscores
  filename <- gsub(" ", "_", filename)
  # Remove or replace special characters (e.g., '+', '/', '\', ':', '*', '?', '"', '<', '>', '|')
  filename <- gsub("[/\\:*?\"<>|+]", "", filename)
  # Optionally remove other non-alphanumeric characters
  filename <- gsub("[^A-Za-z0-9_\\-]", "", filename)
  return(filename)
}

# Create a heatmap from a contingency table with custom axis labels
plot_conti_ht_fx <- function(tbl, xlab, ylab) {
  tmp <- reshape2::melt(tbl)
  p <- ggplot(tmp, aes(x = Var2, y = Var1, fill = value)) +
    geom_tile() +
    scale_fill_gradient(low = "white", high = "blue") +
    labs(x = xlab, y = ylab, fill = "prop") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  return(p)
}

# Function to replace duplicate colors with unique colors
replace_duplicate_colors_fx <- function(color_vector) {
  unique_colors <- unique(color_vector)
  if (length(unique_colors) < length(color_vector)) {
    # Generate a larger set of unique colors to replace duplicates
    all_unique_colors <- colorRampPalette(brewer.pal(12, "Paired"))(length(color_vector) * 2)
    
    # Remove already assigned colors
    remaining_colors <- setdiff(all_unique_colors, unique_colors)
    
    # Replace duplicates with unique colors
    used_colors <- setNames(character(0), character(0))
    for (i in names(color_vector)) {
      if (color_vector[i] %in% used_colors) {
        color_vector[i] <- remaining_colors[1]
        remaining_colors <- remaining_colors[-1]
      }
      used_colors <- c(used_colors, color_vector[i])
    }
  }
  return(color_vector)
}

# Function to generate side by side spatial plots for non-spatial and BANKSY clusters for each ROI
generate_spatial_plots_fx <- function(spe, cnames, color_vector_nsp, color_vector_bank) {
  plot_nsp <- plotColData(spe, x = "x", y = "y", point_size = 0.6, colour_by = cnames[1]) +
    scale_color_manual(values = color_vector_nsp) +
    labs(title = paste0("Non-spatial clusters \n", cell_type_anno, " cells"))
  
  plot_bank <- plotColData(spe, x = "x", y = "y", point_size = 0.6, colour_by = cnames[2]) +
    scale_color_manual(values = color_vector_bank) +
    labs(title = paste0("BANKSY clusters \n", cell_type_anno, " cells"))
  
  return(list(plot_nsp = plot_nsp, plot_bank = plot_bank))
}

rescale_quantiles_asinh_fx <- function(x, cofactor = 1) {
  # Perform the asinh transformation
  x_transformed <- asinh(x / cofactor)
  
  # Determine the 5th and 95th percentiles of the transformed data
  q5 <- quantile(x_transformed, 0.05, na.rm = TRUE)
  q95 <- quantile(x_transformed, 0.95, na.rm = TRUE)
  
  # Rescale values between 0 and 1
  x_rescaled <- (x_transformed - q5) / (q95 - q5)
  
  # Clip values to be within [0, 1]
  x_rescaled[x_rescaled < 0] <- 0
  x_rescaled[x_rescaled > 1] <- 1
  
  return(x_rescaled)
}

# Function to plot violin plot for distribution and spatial at once
plot_violin_spatial_fx <- function(sfe, feature,sample_id="all",size=0.5) {
  violin <- plotColData(sfe, feature, point_fun = function(...) list())
  spatial <- plotSpatialFeature(sfe, feature, colGeometryName = "centroids",
                                scattermore = TRUE,sample_id=sample_id,size=size)
  violin + spatial +
    plot_layout(widths = c(1, 2))
}

# Function to apply thresholds to a single SpatialExperiment object - required for the function apply_thresholds_to_list_fx
apply_thresholds_fx <- function(sfe, donor, dapi_thresholds, area_thresholds) {
  # Extract the thresholds for the current donor
  threshold_DAPI <- unname(dapi_thresholds[donor])
  threshold_Area <- unname(area_thresholds[donor])
  
  # Apply the thresholds to the relevant columns
  pass_DAPI <- sfe$Mean.DAPI > threshold_DAPI
  pass_Area <- sfe$area < threshold_Area
  
  # Create a logical vector for cells that pass both thresholds
  sfe$pass_both_DAPI_area <- pass_DAPI & pass_Area
  
  # Return the updated SpatialExperiment object
  return(sfe)
}

# Function to apply thresholds to a list of SpatialExperiment objects
apply_thresholds_to_list_fx <- function(sfe_list, dapi_thresholds, area_thresholds) {
  # Loop over the list of SpatialExperiment objects and apply the thresholds
  for (i in seq_along(sfe_list)) {
    donor <- names(sfe_list)[i]
    sfe_list[[i]] <- apply_thresholds_fx(sfe_list[[i]], donor, dapi_thresholds, area_thresholds)
  }
  
  return(sfe_list)
}

# Function to exclude non-coding genes from a SpatialFeatureExperiment object based on patterns in the row names
exclude_noncoding_genes_fx <- function(sfe, patterns = c("^Neg", "^SystemControl")) {
  # Defaults to exclude genes starting with "Neg" or "SystemControl" (generated by CosMx)
  
  # Check if 'sfe' has row names
  if (is.null(rownames(sfe))) {
    stop("'sfe' does not have row names. Please assign row names before using this function.")
  }
  
  # Get the number of rows before exclusion
  n_before <- nrow(sfe)
  
  # Combine the patterns into a single regular expression
  combined_pattern <- paste(patterns, collapse = "|")
  
  # Identify rows to exclude based on the combined pattern
  rows_to_exclude <- str_detect(rownames(sfe), combined_pattern)
  
  # Calculate the number of rows to exclude
  n_excluded <- sum(rows_to_exclude)
  
  # Check if any rows match the exclusion criteria
  if (n_excluded > 0) {
    # Subset 'sfe' to exclude the identified rows
    sfe_subset <- sfe[!rows_to_exclude, ]
    
    # Get the number of rows after exclusion
    n_after <- nrow(sfe_subset)
    
    # Inform the user about the exclusion details
    message(sprintf(
      "Excluding non-coding genes:\n- Genes before exclusion: %d\n- Genes excluded: %d\n- Genes after exclusion: %d",
      n_before,
      n_excluded,
      n_after
    ))
    
    # Return the subsetted object
    return(sfe_subset)
  } else {
    # Inform the user that no rows were excluded
    message(sprintf(
      "No genes matching the specified patterns were found in 'sfe'.\n- Genes before: %d\n- Genes after: %d",
      n_before,
      n_before  # Since no rows were excluded
    ))
    
    # Return the original object unchanged
    return(sfe)
  }
}

# Function for checking and optionally fixing compatibility issues in 'colData' slots of S4 objects (DataFrame) when converting to base R data frame
check_coldata_compatibility_fx <- function(
    s4_object,
    auto_convert = FALSE,
    verbose = TRUE
) {
  # Function to check and optionally fix compatibility issues in colData
  
  # Parameters:
  # - s4_object: An S4 object with a 'colData' slot (e.g., SingleCellExperiment)
  # - auto_convert: Logical. If TRUE, attempts to automatically convert problematic columns
  # - verbose: Logical. If TRUE, prints informative messages
  
  # Returns:
  # - A list containing:
  #   - 'problematic_cols': Named list of problematic columns and their classes
  #   - 'coldata_fixed': The (possibly modified) colData as a DataFrame
  
  # Ensure the object has a 'colData' slot
  if (!"colData" %in% slotNames(s4_object)) {
    stop("The provided object does not have a 'colData' slot.")
  }
  
  # Extract 'colData'
  col_data <- colData(s4_object)
  
  # Check if 'colData' is NULL
  if (is.null(col_data)) {
    stop("The 'colData' slot is NULL.")
  }
  
  # Initialize a list to store problematic columns
  problematic_cols <- list()
  
  # Initialize a logical vector to track columns to drop (if necessary)
  cols_to_drop <- logical(length = ncol(col_data))
  names(cols_to_drop) <- colnames(col_data)
  
  # Iterate over each column to check for compatibility issues
  for (col_name in colnames(col_data)) {
    column_data <- col_data[[col_name]]
    col_class <- class(column_data)
    
    # Check for list columns or columns containing S4 objects
    if (is.list(column_data) || isS4(column_data) || any(sapply(column_data, isS4))) {
      problematic_cols[[col_name]] <- col_class
      
      if (auto_convert) {
        # Attempt to flatten list columns with single-element lists
        if (is.list(column_data) && all(sapply(column_data, length) == 1)) {
          col_data[[col_name]] <- unlist(column_data)
          if (verbose) {
            message(sprintf("Auto-converted column '%s' from list to vector.", col_name))
          }
        } else {
          # Mark column for dropping if it cannot be auto-converted
          cols_to_drop[col_name] <- TRUE
          if (verbose) {
            message(sprintf("Column '%s' could not be auto-converted and will be dropped.", col_name))
          }
        }
      }
    }
  }
  
  # Drop problematic columns if auto_convert is TRUE and columns couldn't be fixed
  if (auto_convert && any(cols_to_drop)) {
    col_data <- col_data[, !cols_to_drop, drop = FALSE]
    if (verbose) {
      message("Dropped columns: ", paste(names(cols_to_drop)[cols_to_drop], collapse = ", "))
    }
  }
  
  # Provide messages based on the findings
  if (length(problematic_cols) > 0) {
    if (verbose) {
      message("⚠️ The following columns in 'colData' may cause compatibility issues:")
      for (col in names(problematic_cols)) {
        message(sprintf("- '%s' (class: %s)", col, paste(problematic_cols[[col]], collapse = ", ")))
      }
      if (!auto_convert) {
        message("Consider converting these columns to atomic vectors or handling them before conversion.")
      }
    }
  } else {
    if (verbose) {
      message("✅ No compatibility issues detected in 'colData'.")
    }
  }
  
  # Return a list with problematic columns and the (possibly modified) colData
  return(list(
    problematic_cols = problematic_cols,
    coldata_fixed = col_data
  ))
}

# Function to inspect the distribution of total counts and determine the percentile corresponding to a given floor value
inspect_floor_threshold_fx <- function(counts_matrix, 
                                                floor_value = 20, 
                                                plot = TRUE, 
                                                binwidth = NULL, 
                                                floor_label = NULL,
                                                log_scale = FALSE,
                                                limit_upper_percentile = FALSE) {
  # Function to inspect the distribution of total counts per cell and report the percentile corresponding to a given floor value.
  #
  # Parameters:
  # - counts_matrix: Matrix of counts (genes x cells).
  # - floor_value: Numeric. The floor value to use for normalization (default: 20).
  # - plot: Logical. If TRUE, generates plots and includes them in the output (default: TRUE).
  # - binwidth: Numeric. Width of histogram bins. If NULL, it's automatically calculated.
  # - floor_label: Character. Label for the floor value in plots. If NULL, defaults to "Floor Value: [floor_value]".
  # - log_scale: Logical. If TRUE, plots the total counts on a log scale (default: FALSE).
  # - limit_upper_percentile: Logical. If TRUE, limits the plots to the 95th percentile of total counts (default: FALSE).
  #
  # Returns:
  # - A list containing:
  #   - floor_value: The floor value used.
  #   - floor_percentile: The percentile corresponding to the floor value (as a percentage).
  #   - plots: A list containing the three ggplot objects (histogram, boxplot, CDF plot).
  
  # Load required package
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    install.packages("ggplot2")
  }
  library(ggplot2)
  
  # Ensure counts_matrix is a matrix
  if (!is.matrix(counts_matrix)) {
    counts_matrix <- as.matrix(counts_matrix)
  }
  
  # 1. Calculate total counts per cell
  total_counts <- colSums(counts_matrix)
  
  # 2. Create a data frame for plotting
  counts_df <- data.frame(TotalCounts = total_counts)
  
  # 3. Determine the percentile corresponding to the floor value
  min_total <- min(total_counts)
  max_total <- max(total_counts)
  
  if (floor_value < min_total) {
    warning("The floor_value is less than the minimum total count. Setting floor_percentile to 0.")
    floor_percentile <- 0
  } else if (floor_value > max_total) {
    warning("The floor_value is greater than the maximum total count. Setting floor_percentile to 100.")
    floor_percentile <- 100
  } else {
    # Calculate the percentile
    floor_percentile <- ecdf(total_counts)(floor_value) * 100
  }
  
  # 4. Prepare floor_label if NULL
  if (is.null(floor_label)) {
    floor_label <- paste0("Floor Value: ", floor_value)
  }
  
  # Initialize plot variables
  p1 <- p2 <- p3 <- NULL
  
  # 5. Generate plots with threshold lines if requested
  if (plot) {
    # Optionally limit the data to the 95th percentile
    if (limit_upper_percentile) {
      upper_limit <- quantile(total_counts, 0.95)
      counts_df_plot <- counts_df[counts_df$TotalCounts <= upper_limit, , drop = FALSE]  # Added 'drop = FALSE' here
    } else {
      counts_df_plot <- counts_df
    }
    
    # Calculate default binwidth if not provided
    if (is.null(binwidth)) {
      binwidth <- (max(counts_df_plot$TotalCounts) - min(counts_df_plot$TotalCounts)) / 30
    }
    
    # a. Histogram
    p1 <- ggplot(counts_df_plot, aes(x = TotalCounts)) +
      geom_histogram(binwidth = binwidth, 
                     fill = "steelblue", 
                     color = "black", 
                     alpha = 0.7) +
      geom_vline(xintercept = floor_value, 
                 color = "red", 
                 linetype = "dashed", 
                 size = 1) +
      labs(title = "Histogram of Total Counts per Cell",
           x = "Total Counts",
           y = "Number of Cells") +
      annotate("text", 
               x = floor_value, 
               y = Inf, 
               label = floor_label, 
               vjust = -0.5, 
               hjust = 1.1, 
               color = "red", 
               angle = 90, 
               size = 3.5) +
      theme_minimal()
    
    # Apply log scale if requested
    if (log_scale) {
      p1 <- p1 + scale_x_log10() + labs(x = "Total Counts (log scale)")
    }
    
    # b. Boxplot
    p2 <- ggplot(counts_df_plot, aes(y = TotalCounts)) +
      geom_boxplot(fill = "tomato", 
                   color = "black", 
                   alpha = 0.7) +
      geom_hline(yintercept = floor_value, 
                 color = "red", 
                 linetype = "dashed", 
                 size = 1) +
      labs(title = "Boxplot of Total Counts per Cell",
           y = "Total Counts") +
      annotate("text", 
               x = 1.05, 
               y = floor_value, 
               label = floor_label, 
               color = "red", 
               vjust = -0.5, 
               hjust = 0, 
               size = 3.5) +
      theme_minimal()
    
    # Apply log scale if requested
    if (log_scale) {
      p2 <- p2 + scale_y_log10() + labs(y = "Total Counts (log scale)")
    }
    
    # c. CDF Plot
    p3 <- ggplot(counts_df, aes(x = TotalCounts)) +
      stat_ecdf(geom = "step", 
                color = "purple") +
      geom_vline(xintercept = floor_value, 
                 color = "red", 
                 linetype = "dashed", 
                 size = 1) +
      labs(title = "Cumulative Distribution of Total Counts per Cell",
           x = "Total Counts",
           y = "Proportion of Cells") +
      annotate("text", 
               x = floor_value, 
               y = 1.05, 
               label = floor_label, 
               color = "red", 
               vjust = 0, 
               hjust = 1.1, 
               size = 3.5) +
      theme_minimal()
    
    # Apply log scale and limit x-axis if requested
    if (log_scale) {
      p3 <- p3 + scale_x_log10() + labs(x = "Total Counts (log scale)")
    }
    if (limit_upper_percentile) {
      p3 <- p3 + coord_cartesian(xlim = c(min_total, upper_limit))
    }
  }
  
  # 6. Return results as a list, including plots
  return(list(
    floor_value = floor_value,
    floor_percentile = floor_percentile,
    plots = list(
      histogram = p1,
      boxplot = p2,
      cdf_plot = p3
    )
  ))
}

create_cell_types_annotation_fx <- function(marker_genes, organism, organ) {
  # Load necessary library
  library(dplyr)
  
  # Check if required columns exist in the data frame
  required_columns <- c("gene", "cluster")
  if (!all(required_columns %in% colnames(marker_genes))) {
    stop("The data frame 'marker_genes' must contain 'gene' and 'cluster' columns.")
  }
  
  # Ensure 'cluster' is treated as a factor to maintain order
  marker_genes$cluster <- as.factor(marker_genes$cluster)
  
  # Process the gene lists for each cluster, separating genes with commas
  cell_types_annotation <- marker_genes %>%
    group_by(cluster) %>%
    summarise(genes = paste(gene, collapse = ",")) %>%
    arrange(as.numeric(as.character(cluster))) # Ensure proper ordering
  
  # Create a named vector with names as cluster indices
  cell_types_annotation_vector <- setNames(cell_types_annotation$genes, cell_types_annotation$cluster)
  
  # Construct the initial descriptive message
  initial_message <- paste(
    "Identify cell types of", organism, organ, "cells using the following markers.",
    "Identify one cell type for each row. Only provide the cell type name."
  )
  
  # Combine all gene lists into the message
  gene_lists <- cell_types_annotation$genes
  
  # Combine all parts into the final message
  message_parts <- c(
    initial_message,
    gene_lists,
    "Some can be a mixture of multiple cell types.",
    "Return this as a named vector called cell_types_annotation in R with the names = indices."
  )
  
  # Create the final message string with newline separators
  final_message <- paste(message_parts, collapse = "\n")
  
  # Return both the message and the named vector as a list
  return(list(
    message = final_message,
    cell_types_annotation = cell_types_annotation_vector
  ))
}


#' Find Markers Using Custom BANKSY Function
#'
#' This function identifies marker genes using the Seurat `FindAllMarkers` function
#' on different data matrices derived from a SingleCellExperiment (SCE) object,
#' including BANKSY and smoothed matrices. It allows for customization of parameters
#' and is designed to be generalizable and reproducible.
#'
#' @param spe A `SingleCellExperiment` object containing the data.
#' @param matrix_used A character string specifying which data matrix to use.
#'        Possible values are:
#'        - `"normcounts"`: Use the scaled and normalized counts.
#'        - `"banksy"`: Use the BANKSY matrix.
#'        - `"banksy_smooth"`: Use the smoothed BANKSY matrix.
#'        - `"normcounts_smooth"`: Use the smoothed scaled and normalized counts.
#'        Default is `"normcounts"`.
#' @param ident_banksy Logical value indicating whether to use BANKSY clusters (`TRUE`) or
#'        non-spatial clusters (`FALSE`) for cell identities. Default is `TRUE`.
#' @param level_anno Character or numeric value specifying the level annotation.
#'        Default is `level_anno`.
#' @param repeat_anno Character or numeric value specifying the repeat annotation.
#'        Default is `repeat_anno`.
#'
#' @return A named list containing:
#' \describe{
#'   \item{sce}{The updated `SingleCellExperiment` object with new assays added.}
#'   \item{markers}{A data frame containing all marker genes identified by `FindAllMarkers`.}
#' }
#'
#' @details
#' The function performs the following steps:
#' 1. Constructs the `name_level_repeat` identifier from `level_anno` and `repeat_anno`.
#' 2. Extracts necessary metadata (`rdnames_spe`, `cnames_spe`) from `spe`.
#' 3. Determines the cell identity column and UMAP pattern based on `ident_banksy`.
#' 4. Creates Seurat objects using different data matrices.
#' 5. Generates a smoother based on UMAP embeddings.
#' 6. Applies smoothing to create smoothed assays.
#' 7. Selects the appropriate Seurat object based on `matrix_used`.
#' 8. Identifies marker genes using `FindAllMarkers`.
#' 9. Returns the updated SCE object and marker genes.
#'
#' @examples
#' # Define level and repeat annotations
#' level_anno <- "1"
#' repeat_anno <- "1"
#'
#' # Run the function with specified parameters
#' results <- find_markers_banksy_custom_fx(
#'   spe = spe,
#'   matrix_used = "banksy",
#'   ident_banksy = TRUE,
#'   level_anno = level_anno,
#'   repeat_anno = repeat_anno
#' )
#'
#' # Access the updated SCE object
#' updated_spe <- results$sce
#'
#' # View the marker genes
#' head(results$markers)
#'
#' @export
find_markers_banksy_custom_fx <- function(spe,
                                          matrix_used = "normcounts",
                                          ident_banksy = TRUE,
                                          level_anno = level_anno,
                                          repeat_anno = repeat_anno) {
  # Step 1: Construct name_level_repeat
  name_level_repeat <- paste0("BANKSY_params_level_", level_anno, "_repeat_", repeat_anno)
  
  # Step 2: Extract metadata
  rdnames_spe <- metadata(spe)[[name_level_repeat]]$rdnames_spe
  cnames_spe <- metadata(spe)[[name_level_repeat]]$cnames_spe
  
  # Step 3: Determine identity column and UMAP pattern
  if (ident_banksy) {
    ident_column <- cnames_spe[2]
    umap_pattern <- "^UMAP_M\\d+_lam0\\.\\d+$"
    message(
      "BANKSY clusters were used. The cluster variable is: ",
      ident_column,
      ". The reduced dim variable is: ",
      umap_pattern
    )
  } else {
    ident_column <- cnames_spe[1]
    umap_pattern <- "^UMAP_M\\d+_lam0\\.\\d+$"
    message(
      "Non-spatial clusters were used. The cluster variable is: ",
      ident_column,
      ". The reduced dim variable is: ",
      umap_pattern
    )
  }
  
  # Step 4: Create initial Seurat object with normalized counts
  seurat <- CreateSeuratObject(counts = assays(spe)[["normcounts"]], meta.data = data.frame(colData(spe)[, cnames_spe]))
  # seurat <- as.Seurat(spe) %>% UpdateSeuratObject()
  # seurat@assays$originalexp$data <- assays(spe)[["scalenormcounts"]]
  Idents(seurat) <- seurat[[ident_column]][, 1]
  seurat@assays$RNA$data <- log1p(seurat@assays$RNA$counts) # Fill the data slot with the log counts
  
  # Step 5: Create BANKSY Seurat object
  seurat_bank <- seurat
  seurat_bank@assays$RNA$scale.data <- getBanksyMatrix(spe, M = 1, lambda = 0.2)
  
  # Step 6: Find UMAP embedding based on the pattern
  umap_name <- grep(umap_pattern, rdnames_spe, value = TRUE)
  if (length(umap_name) == 0) {
    stop("No UMAP dimension found matching pattern: ", umap_pattern)
  }

  seurat@reductions[[umap_name]] <- CreateDimReducObject(embeddings = reducedDims(spe)[[umap_name]],
                                                         key = "UMAP_",
                                                         assay = "RNA")
  
  # Step 7: Create smoother using UMAP embeddings
  smoother <- umap_nn_fx(seurat, umap_name, n_neighbors = 500)
  
  # Step 8: Add smoothed assays to the SCE object
  assays(spe, withDimnames = FALSE)[["BANKSY_matrix"]] <- getBanksyMatrix(spe, M = 1, lambda = 0.2)[1:nrow(spe), ]
  assays(spe, withDimnames = FALSE)[["BANKSY_matrix_smooth"]] <- assays(spe)[["BANKSY_matrix"]] %*% smoother
  assays(spe, withDimnames = FALSE)[["normcounts_smooth"]] <- assays(spe)[["normcounts"]] %*% smoother
  
  # Step 9: Create smoothed Seurat objects
  seurat_bank_smooth <- CreateSeuratObject(counts = assays(spe)[["BANKSY_matrix_smooth"]], meta.data = data.frame(colData(spe)[, cnames_spe]))
  seurat_bank_smooth@assays$RNA$scale.data <- seurat_bank_smooth@assays$RNA$counts
  seurat_bank_smooth@assays$RNA$data <- log1p(seurat_bank_smooth@assays$RNA$counts)
  #seurat_bank_smooth <- as.Seurat(spe) %>% UpdateSeuratObject()
  #seurat_bank_smooth@assays$originalexp$data <- as(assays(spe)[["BANKSY_matrix_smooth"]], "dgCMatrix")
  Idents(seurat_bank_smooth) <- seurat_bank_smooth[[ident_column]][, 1]
  
  seurat_smooth <- CreateSeuratObject(counts = assays(spe)[["normcounts_smooth"]], meta.data = data.frame(colData(spe)[, cnames_spe]))
  # seurat_smooth <- as.Seurat(spe) %>% UpdateSeuratObject()
  #  seurat_smooth@assays$originalexp$data <- as(assays(spe)[["scalenormcounts_smooth"]], "dgCMatrix")
  Idents(seurat_smooth) <- seurat_smooth[[ident_column]][, 1]
  seurat_smooth@assays$RNA$data <- log1p(seurat_smooth@assays$RNA$counts)
  
  
  # Step 10: Select the appropriate Seurat object based on matrix_used
  seurat_obj <- switch(
    matrix_used,
    "normcounts" = seurat,
    "normcounts_smooth" = seurat_smooth,
    "banksy" = seurat_bank,
    "banksy_smooth" = seurat_bank_smooth,
    stop(
      "Invalid matrix_used. Must be one of 'normcounts', 'banksy', 'banksy_smooth', 'normcounts_smooth'."
    )
  )
  
  # Step 11: Identify markers using FindAllMarkers
  markers_seurat <- FindAllMarkers(
    object = seurat_obj,
    slot = "data",
    only.pos = TRUE,
    min.pct = 0.1
  )
  
  # Step 12: Clean up Seurat objects to free up resources
  rm(seurat,
     seurat_bank,
     seurat_bank_smooth,
     seurat_smooth,
     seurat_obj)
  gc()  # Suggest garbage collection to free memory
  
  # Step 13: Return the updated SCE object and marker genes as a list
  return(list(sce = spe, markers = markers_seurat))
}

plan_future_fx <- function(memory_fraction = 0.8,
                           handler = "txtprogressbar") {
  # Set up the progress handler based on the input
  if (!is.null(handler)) {
    progressr::handlers(handler)  # Register the specified handler
    message(sprintf("Using '%s' as the progress handler.", handler))
  }
  
  # Determine the operating system
  os_type <- .Platform$OS.type  # "windows" or "unix"
  message(sprintf("Operating system type: %s", os_type))
  
  # Check if running inside RStudio
  in_rstudio <- FALSE
  if (requireNamespace("rstudioapi", quietly = TRUE)) {
    in_rstudio <- rstudioapi::isAvailable()
  }
  message(sprintf("Running in RStudio: %s", in_rstudio))
  
  # Set the parallel plan based on the OS and RStudio
  available_workers <- future::availableCores() - 1  # Use all cores minus one
  available_workers <- max(1, available_workers)  # Ensure at least one worker
  message(sprintf("Available workers: %d", available_workers))
  
  if (os_type == "windows") {
    # Use multisession on Windows
    future::plan("multisession", workers = available_workers)
    message("Using multisession for parallel processing on Windows.")
  } else {
    if (in_rstudio) {
      # If running in RStudio, use multisession to avoid 'multicore' instability
      future::plan("multisession", workers = available_workers)
      message("Running in RStudio. Using multisession for parallel processing on Unix-like OS.")
    } else {
      # Otherwise, use multicore
      future::plan("multicore", workers = available_workers)
      message("Using multicore for parallel processing on Unix-like OS.")
    }
  }
  
  # Initialize variables
  total_physical_bytes <- 0
  total_swap_bytes <- 0
  
  # Get total system memory depending on the OS
  os_type <- Sys.info()["sysname"]
  
  if (os_type == "Windows") {
    # Retrieve total physical memory in bytes using WMIC
    physical_mem_output <- system("wmic computersystem get TotalPhysicalMemory", intern = TRUE)
    total_physical_bytes <- as.numeric(gsub("[^0-9]", "", physical_mem_output[2]))
    message("Retrieved total physical memory for Windows.")
    
    # Retrieve total swap (virtual) memory in bytes using WMIC
    swap_mem_output <- system("wmic pagefile get AllocatedBaseSize", intern = TRUE)
    # The AllocatedBaseSize is in MB, so convert to bytes
    total_swap_bytes <- as.numeric(gsub("[^0-9]", "", swap_mem_output[2])) * 1024^2
    message("Retrieved total swap memory for Windows.")
    
  } else if (os_type == "Darwin") {
    # Retrieve total physical memory in bytes using sysctl
    total_physical_bytes <- as.numeric(system("sysctl -n hw.memsize", intern = TRUE))
    message("Retrieved total physical memory for macOS.")
    
    # Retrieve swap memory using sysctl
    # macOS doesn't provide a straightforward single command to get total swap; 
    # we'll calculate it using vm_stat
    vm_stat <- system("vm_stat", intern = TRUE)
    page_size_line <- grep("page size of", vm_stat, value = TRUE)
    page_size <- as.numeric(sub(".*page size of (\\d+) bytes.*", "\\1", page_size_line))
    
    # Extract swap pages
    swap_pages_used <- as.numeric(gsub("\\.", "", grep("Pages occupied by compressor", vm_stat, value = TRUE)))
    swap_pages_free <- as.numeric(gsub("\\.", "", grep("Pages free", vm_stat, value = TRUE)))
    
    # Calculate total swap in bytes (this is a simplified estimation)
    total_swap_bytes <- (swap_pages_used + swap_pages_free) * page_size
    message("Retrieved total swap memory for macOS (estimated).")
    
  } else {
    # Assume Linux
    # Retrieve total physical memory in bytes using free
    physical_mem_output <- system("free -b | grep Mem", intern = TRUE)
    total_physical_bytes <- as.numeric(strsplit(physical_mem_output, "\\s+")[[1]][2])
    message("Retrieved total physical memory for Linux.")
    
    # Retrieve total swap memory in bytes using free
    swap_mem_output <- system("free -b | grep Swap", intern = TRUE)
    total_swap_bytes <- as.numeric(strsplit(swap_mem_output, "\\s+")[[1]][2])
    message("Retrieved total swap memory for Linux.")
  }
  
  # Calculate the combined total memory (physical + swap)
  total_memory_bytes <- total_physical_bytes + total_swap_bytes
  
  # Calculate the maximum size for globals based on available memory
  max_size_bytes <- total_memory_bytes * memory_fraction
  
  # Set the future.globals.maxSize option based on the calculated memory size
  options(future.globals.maxSize = max_size_bytes)
  
  # Convert to MiB for easier readability
  max_size_mib <- max_size_bytes / (1024 ^ 2)
  message(sprintf("Setting future.globals.maxSize to %.2f MiB", max_size_mib))
}

# Function to check annotations between levels
check_annotations_between_levels_fx <- function(higher_level, lower_level) {
  # Check if higher_level and lower_level exist in 'spe'
  if (!(higher_level %in% colnames(spe))) {
    cat("### Higher level '", higher_level, "' does not exist in 'spe'.\n\n", sep = "")
    return()
  }
  if (!(lower_level %in% colnames(spe))) {
    cat("### Lower level '", lower_level, "' does not exist in 'spe'.\n\n", sep = "")
    return()
  }
  
  unique_higher <- unique(spe[[higher_level]])
  for (anno in unique_higher) {
    # Handle NA values explicitly
    idx <- !is.na(spe[[higher_level]]) & spe[[higher_level]] == anno
    if (any(idx)) {
      associated_lower <- unique(spe[[lower_level]][idx])
      cat("### Cells with ", higher_level, " annotation '", anno, "' have the following ", lower_level, " annotations:\n", sep = "")
      print(associated_lower)
      cat("\n")
    } else {
      cat("### No cells with ", higher_level, " annotation '", anno, "' found.\n\n")
    }
  }
}

# Function to perform replacements and check annotations
perform_replacements_fx <- function(spe, replacements, level_names, level_to_modify_col) {
  for (replacement in replacements) {
    # Extract replacement details
    old_value <- replacement$old_value
    new_value <- replacement$new_value
    target_level <- replacement$level
    new_values_lv <- replacement$new_values_lv
    
    cat("\n----------------------------------------\n")
    cat(sprintf("Processing replacement: '%s' -> '%s' at '%s'\n", old_value, new_value, target_level))
    
    # Display current annotations corresponding to old_value
    for (i in seq_along(level_names)) {
      current_level <- level_names[i]
      current_annotations <- unique(spe[,spe[[level_to_modify_col]] == old_value][[ current_level]])
      
      cat(sprintf("\nLevel %d annotation corresponding to value to change (%s):\n", 
                  i, current_level))
      print(current_annotations)
    }
    
    # Replace old_value with new_value in level_to_modify_col
    spe[[level_to_modify_col]] <- ifelse(spe[[level_to_modify_col]] == old_value, 
                                         new_value, 
                                         spe[[level_to_modify_col]])
    
    # Verify if replacement was successful
    if (!new_value %in% spe[[level_to_modify_col]]) {
      warning(
        sprintf(
          "Replacement failed: '%s' was not found in '%s' after replacement.",
          new_value,
          level_to_modify_col
        )
      )
      next  # Skip to the next replacement
    }
    
    # Determine target level index
    target_index <- match(target_level, level_names)
    if (is.na(target_index)) {
      warning(sprintf("Target level '%s' not found in level_names. Skipping this replacement.", target_level))
      next
    }
    
    # Identify earlier levels
    earlier_indices <- which(seq_along(level_names) < target_index)
    
    # Update earlier levels if new_values_lv is provided
    for (i in earlier_indices) {
      current_level <- level_names[i]
      lv_name <- paste0("lv", i)
      
      if (!is.null(new_values_lv) && lv_name %in% names(new_values_lv)) {
        new_level_value <- new_values_lv[[lv_name]]
        
        # Debugging: Print the condition being applied
        cat(sprintf("\nApplying condition: %s == '%s'\n", level_to_modify_col, new_value))
        
        # Update the current_level with new_level_value where level_to_modify_col == new_value
        spe[[current_level]] <- ifelse(spe[[level_to_modify_col]] == new_value, 
                                       new_level_value, 
                                       spe[[current_level]])
        
        cat(sprintf("Updated %s to '%s' where %s == '%s'\n", 
                    current_level, new_level_value, 
                    level_to_modify_col, new_value))
      } else {
        cat(sprintf("No new value provided for %s. Skipping update for this level.\n", current_level))
      }
    }
    
    # Display updated annotations corresponding to the new value for each level
    for (i in seq_along(level_names)) {
      current_level <- level_names[i]
      updated_annotations <- unique(spe[, spe[[level_to_modify_col]] == new_value][[current_level]])
      
      cat(sprintf("\nLevel %d updated annotation corresponding to value to change (%s):\n", 
                  i, current_level))
      if (length(updated_annotations) > 0) {
        print(updated_annotations)
      } else {
        cat("None\n")
      }
    }
    
    cat("----------------------------------------\n")
  }
  
  return(spe)
}

